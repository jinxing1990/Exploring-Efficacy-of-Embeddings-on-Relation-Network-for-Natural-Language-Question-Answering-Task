{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import callbacks\n",
    "from keras.datasets import mnist\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras import backend as K\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/nvaitc/Documents/repos/ML-RN-Project'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate x_test and y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(995, 190, 1576)\n",
      "(995,)\n"
     ]
    }
   ],
   "source": [
    "RN_input_test = np.load('RN_input_test.npy')\n",
    "a_test = np.load('a_test.npy')\n",
    "print (RN_input_test.shape)\n",
    "print (a_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "[[0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "le_test = preprocessing.LabelEncoder()\n",
    "le_test.fit(a_test)\n",
    "test_labels = le_test.transform(a_test)\n",
    "\n",
    "l = len(np.unique(test_labels))\n",
    "print(l)\n",
    "\n",
    "one_hot_labels = keras.utils.to_categorical(test_labels, num_classes=l)\n",
    "print(one_hot_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(995, 190, 1576)\n",
      "(995, 6)\n"
     ]
    }
   ],
   "source": [
    "x_test = RN_input_test\n",
    "y_test = one_hot_labels\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note:\n",
    "\n",
    "- All models are run with batch size = 64, epoch = 200, validation split = 10% (unless otherwise stated).\n",
    "\n",
    "- Original model: g = [256,256,256,256] and f = [256,512,6] with 50% dropout rate at 2nd layer of f. ReLu activation for all except last layer of f, which is softmax. Before passing the output from g to f, it is summed over 190 combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Comparison between different sample sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(make a table to compare training and validation accuracy)\n",
    "\n",
    "...\n",
    "\n",
    "Embedded c without labels: Validation Accuracy = ~74-75%\n",
    "\n",
    "Embedded c with labels: Validation Accuracy = ~85-85%\n",
    "\n",
    "Conclusion: It is important to include labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparison between different RN inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(make a table to compare training and validation accuracy)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparison between different modification of models [with RN inputs (original)]\n",
    "\n",
    "(a) Original model ( g = [256 x 4], f = [256,512,6] )\n",
    "\n",
    "(b) Increase number of layers in g from 4 to 6 ( g = [256 x 6], f = [256,512,6] )\n",
    "\n",
    "(c) Increase number of layers in g from 4 to 8 ( g = [256 x 8], f = [256,512,6] )\n",
    "\n",
    "(d) Increase number of neurons in each layer of g from 256 to 512 ( g = [512 x 4], f = [256,512,6] )\n",
    "\n",
    "(e) Increase number of neurons in each layer of g from 256 to 512 and 1st layer of f from 256 to 512 ( g = [512 x 4], f = [512,512,6] )\n",
    "\n",
    "(f) Increase number of neurons in each layer of g from 256 to 512. And increase number of layers in g from 4 to 6. ( g = [512 x 6], f = [256,512,6] )\n",
    "\n",
    "(g) Increase number of neurons in each layer of g from 256 to 512 and 1st layer of f from 256 to 512. And increase number of layers in g from 4 to 6. ( g = [512 x 6], f = [512,512,6] )\n",
    "\n",
    "(h) Double the number of neurons in all layers except for the last layer of f. Increase the number of layers of g from 4 to 6. ( g = [512 x 6], f = [512,1024,6] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Original model ( g = [256,256,256,256], f = [256,512,6] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/nvaitc/Documents/repos/ML-RN-Project/model_ori')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load json and create model_ori\n",
    "\n",
    "model_ori_json_file = open(\"model_ori.json\",\"r\")\n",
    "loaded_model_ori_json = model_ori_json_file.read()\n",
    "model_ori_json_file.close()\n",
    "model_ori = model_from_json(loaded_model_ori_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "#load weights into new model_ori\n",
    "\n",
    "model_ori.load_weights(\"W_ori_159_0.87.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy on model_ori: 100%\n",
      "Validation Accuracy on model_ori: 87.1%\n",
      "Test Accuracy on model_ori:  85.5 %\n"
     ]
    }
   ],
   "source": [
    "#evaluate loaded model on test data\n",
    "\n",
    "model_ori.compile(optimizer = Adam(lr = 2e-4), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "score_ori = model_ori.evaluate(x_test,y_test,verbose = 0)\n",
    "print(\"Training Accuracy on model_ori: 100%\")\n",
    "print(\"Validation Accuracy on model_ori: 87.1%\")\n",
    "print(\"Test Accuracy on model_ori: \", \"{0:0.1f}\".format(score_ori[1]*100), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/nvaitc/Documents/repos/ML-RN-Project')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Increase number of layers in g from 4 to 6 ( g = [256,256,256,256,256,256], f = [256,512,6] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/nvaitc/Documents/repos/ML-RN-Project/model_g6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load json and create model_ori\n",
    "\n",
    "model_g6_json_file = open(\"model_g6.json\",\"r\")\n",
    "loaded_model_g6_json = model_g6_json_file.read()\n",
    "model_g6_json_file.close()\n",
    "model_g6 = model_from_json(loaded_model_g6_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "#load weights into new model_ori\n",
    "\n",
    "model_g6.load_weights(\"W_g6_188_0.89.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy on model_g6: 100%\n",
      "Validation Accuracy on model_g6: 89.1%\n",
      "Test Accuracy on model_g6:  88.4 %\n"
     ]
    }
   ],
   "source": [
    "#evaluate loaded model on test data\n",
    "\n",
    "model_g6.compile(optimizer = Adam(lr = 2e-4), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "score_g6 = model_g6.evaluate(x_test,y_test,verbose = 0)\n",
    "print(\"Training Accuracy on model_g6: 100%\")\n",
    "print(\"Validation Accuracy on model_g6: 89.1%\")\n",
    "print(\"Test Accuracy on model_g6: \", \"{0:0.1f}\".format(score_g6[1]*100), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/nvaitc/Documents/repos/ML-RN-Project')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Increase number of layers in g from 4 to 8 ( g = [256 x 8], f = [256,512,6] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/nvaitc/Documents/repos/ML-RN-Project/model_g8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load json and create model_ori\n",
    "\n",
    "model_g8_json_file = open(\"model_g8.json\",\"r\")\n",
    "loaded_model_g8_json = model_g8_json_file.read()\n",
    "model_g8_json_file.close()\n",
    "model_g8 = model_from_json(loaded_model_g8_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "#load weights into new model_ori\n",
    "\n",
    "model_g8.load_weights(\"W_g8_05_0.22.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy on model_g8: 18.4%\n",
      "Validation Accuracy on model_g8: 22.2%\n",
      "Test Accuracy on model_g8:  20.0 %\n"
     ]
    }
   ],
   "source": [
    "#evaluate loaded model on test data\n",
    "\n",
    "model_g8.compile(optimizer = Adam(lr = 2e-4), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "score_g8 = model_g8.evaluate(x_test,y_test,verbose = 0)\n",
    "print(\"Training Accuracy on model_g8: 18.4%\")\n",
    "print(\"Validation Accuracy on model_g8: 22.2%\")\n",
    "print(\"Test Accuracy on model_g8: \", \"{0:0.1f}\".format(score_g8[1]*100), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/nvaitc/Documents/repos/ML-RN-Project')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Increase number of neurons in each layer of g from 256 to 512 ( g = [512 x 4], f = [256,512,6] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/nvaitc/Documents/repos/ML-RN-Project/model_512')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load json and create model_ori\n",
    "\n",
    "model_512_json_file = open(\"model_512.json\",\"r\")\n",
    "loaded_model_512_json = model_512_json_file.read()\n",
    "model_512_json_file.close()\n",
    "model_512 = model_from_json(loaded_model_512_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "#load weights into new model_ori\n",
    "\n",
    "model_512.load_weights(\"W_512_179_0.88.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy on model_512: 100%\n",
      "Validation Accuracy on model_512: 88.2%\n",
      "Test Accuracy on model_512:  85.0 %\n"
     ]
    }
   ],
   "source": [
    "#evaluate loaded model on test data\n",
    "\n",
    "model_512.compile(optimizer = Adam(lr = 2e-4), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "score_512 = model_512.evaluate(x_test,y_test,verbose = 0)\n",
    "print(\"Training Accuracy on model_512: 100%\")\n",
    "print(\"Validation Accuracy on model_512: 88.2%\")\n",
    "print(\"Test Accuracy on model_512: \", \"{0:0.1f}\".format(score_512[1]*100), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/nvaitc/Documents/repos/ML-RN-Project')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) Increase number of neurons in each layer of g from 256 to 512 and 1st layer of from 256 to 512 ( g = [512 x 4], f = [512,512,6] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/nvaitc/Documents/repos/ML-RN-Project/model_512f512')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load json and create model_ori\n",
    "\n",
    "model_512f512_json_file = open(\"model_512f512.json\",\"r\")\n",
    "loaded_model_512f512_json = model_512f512_json_file.read()\n",
    "model_512f512_json_file.close()\n",
    "model_512f512 = model_from_json(loaded_model_512f512_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "#load weights into new model_ori\n",
    "\n",
    "model_512f512.load_weights(\"W_512f512_193_0.88.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy on model_512f512: 99.9%\n",
      "Validation Accuracy on model_512f512: 87.9%\n",
      "Test Accuracy on model_512f512:  88.3 %\n"
     ]
    }
   ],
   "source": [
    "#evaluate loaded model on test data\n",
    "\n",
    "model_512f512.compile(optimizer = Adam(lr = 2e-4), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "score_512f512 = model_512f512.evaluate(x_test,y_test,verbose = 0)\n",
    "print(\"Training Accuracy on model_512f512: 99.9%\")\n",
    "print(\"Validation Accuracy on model_512f512: 87.9%\")\n",
    "print(\"Test Accuracy on model_512f512: \", \"{0:0.1f}\".format(score_512f512[1]*100), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/nvaitc/Documents/repos/ML-RN-Project')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (f) Increase number of neurons in each layer of g from 256 to 512. And increase number of layers in g from 4 to 6. ( g = [512 x 6], f = [256,512,6] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/nvaitc/Documents/repos/ML-RN-Project/model_g6x512')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load json and create model_ori\n",
    "\n",
    "model_g6x512_json_file = open(\"model_g6x512.json\",\"r\")\n",
    "loaded_model_g6x512_json = model_g6x512_json_file.read()\n",
    "model_g6x512_json_file.close()\n",
    "model_g6x512 = model_from_json(loaded_model_g6x512_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "#load weights into new model_ori\n",
    "\n",
    "model_g6x512.load_weights(\"W_g6x512_164_0.88.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy on model_g6x512: 100%\n",
      "Validation Accuracy on model_g6x512: 87.8%\n",
      "Test Accuracy on model_g6x512:  88.5 %\n"
     ]
    }
   ],
   "source": [
    "#evaluate loaded model on test data\n",
    "\n",
    "model_g6x512.compile(optimizer = Adam(lr = 2e-4), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "score_g6x512 = model_g6x512.evaluate(x_test,y_test,verbose = 0)\n",
    "print(\"Training Accuracy on model_g6x512: 100%\")\n",
    "print(\"Validation Accuracy on model_g6x512: 87.8%\")\n",
    "print(\"Test Accuracy on model_g6x512: \", \"{0:0.1f}\".format(score_g6x512[1]*100), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/nvaitc/Documents/repos/ML-RN-Project')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (g) Increase number of neurons in each layer of g from 256 to 512 and 1st layer of f from 256 to 512. And increase number of layers in g from 4 to 6. ( g = [512 x 6], f = [512,512,6] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/nvaitc/Documents/repos/ML-RN-Project/model_g6x512f512')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load json and create model_ori\n",
    "\n",
    "model_g6x512f512_json_file = open(\"model_g6x512f512.json\",\"r\")\n",
    "loaded_model_g6x512f512_json = model_g6x512f512_json_file.read()\n",
    "model_g6x512f512_json_file.close()\n",
    "model_g6x512f512 = model_from_json(loaded_model_g6x512f512_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "#load weights into new model_ori\n",
    "\n",
    "model_g6x512f512.load_weights(\"W_g6x512f512_179_0.88.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy on model_g6x512f512: 99.7%\n",
      "Validation Accuracy on model_g6x512f512: 88.3%\n",
      "Test Accuracy on model_g6x512f512:  89.0 %\n"
     ]
    }
   ],
   "source": [
    "#evaluate loaded model on test data\n",
    "\n",
    "model_g6x512f512.compile(optimizer = Adam(lr = 2e-4), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "score_g6x512f512 = model_g6x512f512.evaluate(x_test,y_test,verbose = 0)\n",
    "print(\"Training Accuracy on model_g6x512f512: 99.7%\")\n",
    "print(\"Validation Accuracy on model_g6x512f512: 88.3%\")\n",
    "print(\"Test Accuracy on model_g6x512f512: \", \"{0:0.1f}\".format(score_g6x512f512[1]*100), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/nvaitc/Documents/repos/ML-RN-Project')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (h) Double the number of neurons in all layers except for the last layer of f. Increase the number of layers of g from 4 to 6. ( g = [512 x 6], f = [512,1024,6] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/nvaitc/Documents/repos/ML-RN-Project/model_double')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load json and create model_ori\n",
    "\n",
    "model_double_json_file = open(\"model_double.json\",\"r\")\n",
    "loaded_model_double_json = model_double_json_file.read()\n",
    "model_double_json_file.close()\n",
    "model_double = model_from_json(loaded_model_double_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "#load weights into new model_ori\n",
    "\n",
    "model_double.load_weights(\"W_double_177_0.88.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy on model_double: 100%\n",
      "Validation Accuracy on model_double: 88.2%\n",
      "Test Accuracy on model_double:  88.4 %\n"
     ]
    }
   ],
   "source": [
    "#evaluate loaded model on test data\n",
    "\n",
    "model_double.compile(optimizer = Adam(lr = 2e-4), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "score_double = model_double.evaluate(x_test,y_test,verbose = 0)\n",
    "print(\"Training Accuracy on model_double: 100%\")\n",
    "print(\"Validation Accuracy on model_double: 88.2%\")\n",
    "print(\"Test Accuracy on model_double: \", \"{0:0.1f}\".format(score_double[1]*100), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments and Results\n",
    "\n",
    "\n",
    "\n",
    "We have made several comparison between different types of embeddings, different sample sizes, different types of inputs to feed to the RN module and different modifications to the RN module with our implementation of USE embeddings. We compared the accuracy of the test set for different modifications to the RN module and compared the accuracy of the validation set for the rest of the comparisons.\n",
    "\n",
    "For the experiments, all models were run with batch size of 64, with 200 epochs and validation set was taken from the last 10\\% of the training set. For the original RN module, $f_\\phi \\left(\\sum_{i,j} g_\\theta (o_i,o_j,q)\\right)$, $g_\\theta$ is a four-layer MLP 256 unites per layer and $f_\\phi$ is a three-layer MLP consisting of 256, 512 and 6 units, where the final softmax output was optimized with a cross-entropy loss function using the Adam optimizer with a learning rate of $2e^{-4}$. All models are run on original RN module except for the different modifications to the RN module. The results and weights used are all based on the epoch that gave the best validation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison between Different Embeddings\n",
    "\n",
    "\n",
    "\n",
    "We have made comparisons between the two different embeddings of the sentences of the contexts, questions and answers from all data from the training set of Task 2:\n",
    "\n",
    "- Model LSTM: Use LSTM to embed the sentences of the contexts, questions and answers, and then to pass to the RN module\n",
    "- Model USE: Use USE to embed the sentences of the contexts, questions and answers, and then to pass to the RN module\n",
    "\n",
    "|Model|Training Accuracy (%)|Validation Accuracy (%)|\n",
    "|---|---|---|\n",
    "|LSTM|---|---|\n",
    "|USE|100|87.1|\n",
    "\n",
    "$$\n",
    "\n",
    "#for latex\n",
    "\n",
    "\\begin{center}\n",
    "\\begin{tabular}{|r|r|r|}\n",
    "\n",
    "\\hline\n",
    "\\textbf{Model}&\\textbf{Training Accuracy (\\%)}&\\textbf{Training Accuracy (\\%)}\\\\\n",
    "\\hline\n",
    "LSTM & &\\\\\n",
    "\\hline\n",
    "USE & & \\\\\n",
    "\\hline\n",
    "\n",
    "\\end{tabular}\n",
    "\\end{center}\n",
    "\n",
    "$$\n",
    "\n",
    "From the results, we can see that based on training solely on the training set of Task 2, using USE embedding provides a much better representation than LSTM embedding in training our RN module. Thus, we will be running the rest of the models with USE embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison between Different Sample Sizes\n",
    "\n",
    "\n",
    "\n",
    "We ran the original model and compared between 2 different sample sizes of the training set:\n",
    "\n",
    "- First 1000 contexts of the training set\n",
    "- Full 9995 contexts of the training set\n",
    "\n",
    "|Sample|Training Accuracy (%)|Validation Accuracy (%)|\n",
    "|---|---|---|\n",
    "|First 1000|96.2|35.0|\n",
    "|Full 9995|100|87.1|\n",
    "\n",
    "$$\n",
    "\n",
    "#for latex\n",
    "\n",
    "\\begin{center}\n",
    "\\begin{tabular}{|r|r|r|}\n",
    "\n",
    "\\hline\n",
    "\\textbf{Sample}&\\textbf{Training Accuracy (\\%)}&\\textbf{Training Accuracy (\\%)}\\\\\n",
    "\\hline\n",
    "First 1000 & 96.2 & 35.0\\\\\n",
    "\\hline\n",
    "Full 9995 & 100 & 87.1\\\\\n",
    "\\hline\n",
    "\n",
    "\\end{tabular}\n",
    "\\end{center}\n",
    "\n",
    "$$\n",
    "\n",
    "Based on the validation accuracy, we can see that it is necessary to include all data in the training set for training of the models. Thus, we will be running the rest of the models based on the full 9995 contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison between Different RN inputs\n",
    "\n",
    "\n",
    "\n",
    "We made comparison between different modifications of inputs before passing to the RN module:\n",
    "\n",
    "- RN inputs with no labels: Removed the index of every sentence of each context prior to each question\n",
    "- RN inputs with labels: Includes both the sentence embeddings and index of every sentence of each context prior to each question (as of what was implemented in the Santoro paper)\n",
    "\n",
    "|RN inputs|Training Accuracy (%)|Validation Accuracy (%)|\n",
    "|---|---|---|\n",
    "|Without Index|100|76.9|\n",
    "|With Index|100|87.1|\n",
    "\n",
    "$$\n",
    "\n",
    "#for latex\n",
    "\n",
    "\\begin{center}\n",
    "\\begin{tabular}{|r|r|r|}\n",
    "\n",
    "\\hline\n",
    "\\textbf{RN inputs}&\\textbf{Training Accuracy (\\%)}&\\textbf{Validation Accuracy (\\%)}\\\\\n",
    "\\hline\n",
    "Without Index & 100 & 76.9\\\\\n",
    "\\hline\n",
    "With Index & 100 & 87.1\\\\\n",
    "\\hline\n",
    "\n",
    "\\end{tabular}\n",
    "\\end{center}\n",
    "\n",
    "$$\n",
    "\n",
    "Based on the validation accuracies, it seems that the indexing of the sentences in the contexts prior to each question serves an important attribute for the RN module to train on. Thus. we included indexing for the RN inputs for rest of the models that would be implemented. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison between Different Modification of the RN Module\n",
    "\n",
    "\n",
    "\n",
    "For LSTM embedding, each sentence and question had been embedded to a 32-unit object, with sentences from the contexts and questions being processed through separate LSTMs. For each question embedding, it is paired with a combination of 2 sentence embeddings from up to 20 prior sentences, with indexes. Batches of 190 (20 sentences choose 2) combinations of dimension 136 vectors (32-unit sentence i embedding + one hot 20-dimension vector for index of sentence i + 32-unit sentence j embedding + one hot 20-dimension vector for index of sentence j + 32-unit question embedding) with batch size of 64 were passed through the RN module.\n",
    "\n",
    "On the other hand, each sentence and question had been embedded to a 512-unit object with the pre-trained USE. As a result, instead of having 190 combinations of dimension 136 vectors with batch size of 64 being passed through the RN module, we would have combinations of dimension 1576 (512-unit sentence i embedding + one hot 20-dimension vector for index of sentence i + 512-unit sentence j embedding + one hot 20-dimension vector for index of sentence j + 512-unit question embedding). From this we can see that there is an increase in the dimension of each input, and hence, the complexity of each input before passing through the RN module. Thus, we believed by increasing the number of layers or/and number of neurons in each layer would improve the accuracy of the test set as the RN module would have more components to learn from.\n",
    "\n",
    "We compared between different modifications of the RN model and checked for the accuracy of the test set:\n",
    "\n",
    "- Model ori: Original RN module (g: [256 x 4], f: [256,512,6])\n",
    "- Model g6: Increase number of layers of g from 4 to 6 (g: [256 x 6], f: [256,512,6])\n",
    "- Model g8: Increase number of layers of g from 4 to 8 (g: [256 x 8], f: [256,512,6])\n",
    "- Model 512: Increase number of neurons in each layer of g from 256 to 512 (g: [512 x 4], f: [256,512,6])\n",
    "- Model 512f512: Increase number of neurons in each layer of g from 256 to 512 and first layer of f from 256 to 512 (g: [512 x 4], f: [512,512,6])\n",
    "- Model g6x512: Increase number of layers of g from 4 to 6 and increase number of neurons in each layer of g from 256 to 512 (g: [512 x 6], f: [256,512,6])\n",
    "- Model g6x512f512: Increase number of layers of g from 4 to 6, increase number of neurons in each layer of g from 256 to 512 and first layer of f from 256 to 512 (g: [512 x 6], f: [512,512,6])\n",
    "- Model double: Increase number of layers of g from 4 to 6 and double the number of neurons in all layers except for the last layer of f (g: [512 x 6], f: [512,1024,6])\n",
    "\n",
    "|Model|Training Accuracy (\\%)|Validation Accuracy (\\%)|Test Accuracy (\\%)|\n",
    "|---|---|---|---|\n",
    "|ori|100|87.1|85.5|\n",
    "|g6|100|89.1|88.4|\n",
    "|g8|18.4|22.2|20.0|\n",
    "|512|100|88.2|85.0|\n",
    "|512f512|99.9|87.9|88.3|\n",
    "|g6x512|100|87.8|88.5|\n",
    "|g6x512f512|99.7|88.3|89.0|\n",
    "|double|100|88.2|88.4|\n",
    "\n",
    "$$\n",
    "\n",
    "#for latex\n",
    "\n",
    "\\begin{center}\n",
    "\\begin{tabular}{|r|r|r|r|}\n",
    "\n",
    "\\hline\n",
    "\\textbf{Model}&\\textbf{Training Accuracy (\\%)|&\\textbf{Validation Accuracy (\\%)}&\\textbf{Test Accuracy (\\%)}\\\\\n",
    "\\hline\n",
    "ori&100&87.1&85.5\\\\\n",
    "\\hline\n",
    "g6&100&89.1&88.4\\\\\n",
    "\\hline\n",
    "g8&18.4&22.2&20.0\\\\\n",
    "\\hline\n",
    "512&100&88.2&85.0\\\\\n",
    "\\hline\n",
    "512f512&99.9&87.9&88.3\\\\\n",
    "\\hline\n",
    "g6x512&100&87.8&88.5\\\\\n",
    "\\hline\n",
    "g6x512f512&99.7&88.3&89.0\\\\\n",
    "\\hline\n",
    "double&100&88.2&88.4\\\\\n",
    "\\hline\n",
    "\n",
    "\\end{tabular}\n",
    "\\end{center}\n",
    "\n",
    "$$\n",
    "\n",
    "Based on the test accuracies of the above models, we can see that increasing the number of layers of g function from 4 to 6 increased the test accuracy but drastically decreased the accuracy when it was increased to 8 layers. There is also improvement in the test accuracy when the number of neurons for each layer of g function from 256 to 512. The above table shows that the best model for our USE implementation is Model g6x512f512, which is to increase both the number of layers from 4 to 6 and the number of neurons from 256 to 512 for each layer of the g function, together with an increase of number of neurons from 256 to 512 for the first layer of the f function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
